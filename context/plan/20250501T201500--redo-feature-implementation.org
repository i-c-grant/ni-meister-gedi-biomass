#+title:      Redo Feature Implementation Plan
#+date:       [2025-05-01 Thu 20:15]
#+filetags:   :nimeistergedibiomassglobal:project:implementation:
#+identifier: 20250501T201500

* Implementation Overview
** Objective
Add --redo option to run_on_maap.py to exclude previously successful granules from new runs

** Key Components
1. New CLI options: --redo and --force-redo
2. S3 output path construction logic
3. Granule ID extraction from output filenames
4. Existing output detection and exclusion
5. Safety checks for tag conflicts

* Dependencies
- MAAP API access
- boto3 S3 client configuration
- Existing filename/granule matching logic

* Step-by-Step Implementation Plan

** 1. Add New Command Line Options
#+begin_src python
@click.option("--redo", type=str, help="Tag of previous run to exclude")
@click.option("--force-redo", is_flag=True, help="Allow redo with same tag")
#+end_src

** 2. Filename Parsing Utilities
#+begin_src python
def get_key_from_output(filename: str) -> str:
    """Extract base key from output filename"""
    return filename.split(".")[0].strip()
#+end_src

** 3. S3 Output Querying
Note: Output filenames follow pattern: YYYYDDDHHMMSS_ORNNNN_XX.gpkg.bz2 where:
- YYYYDDDHHMMSS: Acquisition datetime
- ORNNNN: Orbit number
- XX: Granule number
This matches the GEDI L1B/L2A/L4A granule naming convention.

#+begin_src python
def get_existing_outputs(username: str, algo_id: str, 
                         version: str, redo_tag: str) -> Set[str]:
    """Get set of processed output keys from previous run"""
    s3 = boto3.client('s3')
    existing = set()
    
    paginator = s3.get_paginator('list_objects_v2')
    for page in paginator.paginate(
        Bucket="maap-ops-workspace",
        Prefix=f"{username}/dps_output/{algo_id}/{version}/{redo_tag}/"
    ):
        for obj in page.get('Contents', []):
            if obj['Key'].endswith('.gpkg.bz2'):
                key = get_key_from_output(Path(obj['Key']).name)
                existing.add(key)
                
    return existing
#+end_src

** 4. Granule Filtering Integration
Modify main() workflow to filter using existing output keys:

1. After loading existing keys from --redo tag:
#+begin_src python
# Convert granule triplets to their stripped names
granule_keys = {
    stripped_granule_name(granule[prod])
    for matched in matched_granules
    for prod in ['l1b', 'l2a', 'l4a']
}

# Filter out any triplet where any granule contains an existing key
matched_granules = [
    matched
    for matched in matched_granules
    if not any(
        any(key in stripped_granule_name(matched[prod])  # Substring match
        for key in existing_keys
        for prod in ['l1b', 'l2a', 'l4a']
    )
]
#+end_src

2. Replace existing exclude_path logic with redo-based exclusion:
#+begin_src python
# Old exclude_path logic
if exclude_path:
    with open(s3_url_to_local_path(exclude_path), "r") as f:
        excluded_granules = [line.strip() for line in f]
        
# Combine exclusions with existing_path if provided
excluded_granules = []
if exclude_path:
    with open(s3_url_to_local_path(exclude_path), "r") as f:
        excluded_granules.extend(line.strip() for line in f)

if redo_tag:  # Takes priority
    existing_keys = get_existing_outputs(username, algo_id, algo_version, redo_tag)
    excluded_granules.extend(existing_keys)
   
** 5. Safety Checks
#+begin_src python
if redo_tag:
    if not force_redo and redo_tag == tag:
        raise ValueError(f"Cannot redo with same tag '{tag}'")
    # Verify S3 path exists before checking contents
    s3 = boto3.client('s3')
    prefix = f"{username}/dps_output/{algo_id}/{algo_version}/{redo_tag}/"
    result = s3.list_objects_v2(Bucket="maap-ops-workspace", Prefix=prefix, MaxKeys=1)
    if not result.get('KeyCount'):
        raise ValueError(f"No output directory found for redo tag '{redo_tag}'")
        
    existing = get_existing_outputs(username, algo_id, algo_version, redo_tag)
    if not existing:
        raise ValueError(f"Redo tag '{redo_tag}' exists but contains no valid outputs")
#+end_src

